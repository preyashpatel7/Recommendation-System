{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8365f3-775c-4d13-8843-69c68489c6d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q tf-agents==0.19.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8630a13-512e-46af-8d86-106c5d3b0778",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tf_agents.bandits.agents import dropout_thompson_sampling_agent as dropout_ts_agent\n",
    "from tf_agents.bandits.agents import lin_ucb_agent\n",
    "from tf_agents.bandits.agents import linear_thompson_sampling_agent as lin_ts_agent\n",
    "from tf_agents.bandits.agents import neural_epsilon_greedy_agent as eps_greedy_agent\n",
    "from tf_agents.bandits.agents.examples.v2 import trainer\n",
    "from tf_agents.bandits.environments import environment_utilities\n",
    "from tf_agents.bandits.environments import movielens_per_arm_py_environment\n",
    "from tf_agents.bandits.environments import movielens_py_environment\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.bandits.networks import global_and_arm_feature_network\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import q_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b78d9377-3d3c-48c0-9b7a-5e9cc200bbcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "BATCH_SIZE = 8\n",
    "TRAINING_LOOPS = 20000\n",
    "STEPS_PER_LOOP = 2\n",
    "\n",
    "RANK_K = 20\n",
    "NUM_ACTIONS = 20\n",
    "\n",
    "AGENT_ALPHA = 10.0\n",
    "EPSILON = 0.05\n",
    "LAYERS = (50, 50, 50)\n",
    "LR = 0.005\n",
    "DROPOUT_RATE = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b599547-7370-420e-b354-6fe80b64067e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_bandit_training(root_dir, data_path, agent_name=\"LinUCB\", per_arm=False):\n",
    "    tf.compat.v1.enable_v2_behavior()\n",
    "\n",
    "    if not data_path:\n",
    "        raise ValueError('Please specify the location of the MovieLens data file.')\n",
    "\n",
    "    # Environment\n",
    "    if per_arm:\n",
    "        env = movielens_per_arm_py_environment.MovieLensPerArmPyEnvironment(\n",
    "            data_path, RANK_K, BATCH_SIZE, num_actions=NUM_ACTIONS, csv_delimiter='\\t')\n",
    "    else:\n",
    "        env = movielens_py_environment.MovieLensPyEnvironment(\n",
    "            data_path, RANK_K, BATCH_SIZE, num_movies=NUM_ACTIONS, csv_delimiter='\\t')\n",
    "\n",
    "    environment = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "    # Optimal reward/action functions\n",
    "    optimal_reward_fn = functools.partial(\n",
    "        environment_utilities.compute_optimal_reward_with_movielens_environment,\n",
    "        environment=environment)\n",
    "    optimal_action_fn = functools.partial(\n",
    "        environment_utilities.compute_optimal_action_with_movielens_environment,\n",
    "        environment=environment)\n",
    "\n",
    "    # Agent selection\n",
    "    if agent_name == 'LinUCB':\n",
    "        agent = lin_ucb_agent.LinearUCBAgent(\n",
    "            time_step_spec=environment.time_step_spec(),\n",
    "            action_spec=environment.action_spec(),\n",
    "            tikhonov_weight=0.001,\n",
    "            alpha=AGENT_ALPHA,\n",
    "            dtype=tf.float32,\n",
    "            accepts_per_arm_features=per_arm,\n",
    "        )\n",
    "    elif agent_name == 'LinTS':\n",
    "        agent = lin_ts_agent.LinearThompsonSamplingAgent(\n",
    "            time_step_spec=environment.time_step_spec(),\n",
    "            action_spec=environment.action_spec(),\n",
    "            dtype=tf.float32,\n",
    "            accepts_per_arm_features=per_arm,\n",
    "        )\n",
    "    elif agent_name == 'epsGreedy':\n",
    "        if per_arm:\n",
    "            network = global_and_arm_feature_network.create_feed_forward_dot_product_network(\n",
    "                environment.time_step_spec().observation,\n",
    "                global_layers=LAYERS,\n",
    "                arm_layers=LAYERS,\n",
    "            )\n",
    "        else:\n",
    "            network = q_network.QNetwork(\n",
    "                input_tensor_spec=environment.time_step_spec().observation,\n",
    "                action_spec=environment.action_spec(),\n",
    "                fc_layer_params=LAYERS,\n",
    "            )\n",
    "        agent = eps_greedy_agent.NeuralEpsilonGreedyAgent(\n",
    "            time_step_spec=environment.time_step_spec(),\n",
    "            action_spec=environment.action_spec(),\n",
    "            reward_network=network,\n",
    "            optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=LR),\n",
    "            epsilon=EPSILON,\n",
    "            emit_policy_info='predicted_rewards_mean',\n",
    "            info_fields_to_inherit_from_greedy=['predicted_rewards_mean'],\n",
    "        )\n",
    "    elif agent_name == 'DropoutTS':\n",
    "        train_step_counter = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "        def dropout_fn():\n",
    "            return tf.math.maximum(\n",
    "                tf.math.reciprocal_no_nan(1.01 + tf.cast(train_step_counter, tf.float32)),\n",
    "                0.0003,\n",
    "            )\n",
    "\n",
    "        agent = dropout_ts_agent.DropoutThompsonSamplingAgent(\n",
    "            time_step_spec=environment.time_step_spec(),\n",
    "            action_spec=environment.action_spec(),\n",
    "            dropout_rate=dropout_fn,\n",
    "            network_layers=LAYERS,\n",
    "            optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=LR),\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown agent: {agent_name}\")\n",
    "\n",
    "    # Metrics\n",
    "    regret_metric = tf_bandit_metrics.RegretMetric(optimal_reward_fn)\n",
    "    suboptimal_arms_metric = tf_bandit_metrics.SuboptimalArmsMetric(optimal_action_fn)\n",
    "    \n",
    "    # Train the agent\n",
    "    trainer.train(\n",
    "        root_dir=root_dir,\n",
    "        agent=agent,\n",
    "        environment=environment,\n",
    "        training_loops=TRAINING_LOOPS,\n",
    "        steps_per_loop=STEPS_PER_LOOP,\n",
    "        additional_metrics=[regret_metric, suboptimal_arms_metric],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab889aa-87b3-4e34-8ea2-afa4cb0d085b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    run_bandit_training(\n",
    "    root_dir='/tmp/movielens_bandit_output',\n",
    "    data_path='u.data',\n",
    "    agent_name='LinUCB',  # Options: LinUCB, LinTS, epsGreedy, DropoutTS\n",
    "    per_arm=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf1a3aa-8d79-475e-85bb-7202cebbd105",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /tmp/movielens_bandit_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e320d58f-83ff-46a2-95bf-3437e0b385cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
